{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from rouge import Rouge\n",
    "import string, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hospitalityData(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        squad = json.load(f)\n",
    "\n",
    "    # initialize list where we will place all of our data\n",
    "    hospitality_jum = []\n",
    "    \n",
    "    for group in squad['data']:\n",
    "        for paragraph in group['paragraphs']:\n",
    "            # we pull out the context from here\n",
    "            context = paragraph['context']\n",
    "            for qa_pair in paragraph['qas']:\n",
    "                # we pull out the question\n",
    "                question = qa_pair['question']\n",
    "                # now the logic to check if we have 'answers' or 'plausible_answers'\n",
    "                if 'answers' in qa_pair.keys() and len(qa_pair['answers']) > 0:\n",
    "                    answer = qa_pair['answers'][0]['text']\n",
    "                elif 'plausible_answers' in qa_pair.keys() and len(qa_pair['plausible_answers']) > 0:\n",
    "                    answer = qa_pair['plausible_answers'][0]['text']\n",
    "                else:\n",
    "                    # this shouldn't happen, but just in case we just set answer = None\n",
    "                    answer = None\n",
    "                # append dictionary sample to parsed squad\n",
    "                hospitality_jum.append({\n",
    "                    'question': question,\n",
    "                    'answer': answer,\n",
    "                    'context': context\n",
    "                })\n",
    "    # return formatted data lists\n",
    "    return hospitality_jum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "hospitalityDataset = read_hospitalityData('data/f21mp_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for cleaning the prediction values\n",
    "def normalize_text(s):\n",
    "    \"\"\"Typically, text processing steps include removing articles and punctuation and standardizing whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "# Method to compute  Avg EM evaluation\n",
    "def compute_exact_match(model_out,reference):\n",
    "    em = []    \n",
    "    for i in range(len(model_out)):\n",
    "        if normalize_text(model_out[i]) == normalize_text(reference[i]):\n",
    "            em.append(1)\n",
    "        else:\n",
    "            em.append(0)    \n",
    "    return sum(em)/len(em)\n",
    "# Method to compute EM for each prediction\n",
    "def compute_exact_match_eachPredictions(model_out,reference):\n",
    "    em = []    \n",
    "    for i in range(len(model_out)):\n",
    "        if normalize_text(model_out[i]) == normalize_text(reference[i]):\n",
    "            em.append([normalize_text(model_out[i]),normalize_text(reference[i]),1])\n",
    "        else:\n",
    "            em.append([normalize_text(model_out[i]),normalize_text(reference[i]),0])  \n",
    "    return em\n",
    "\n",
    "def compute_rouge_evaulation(model_out,reference):    \n",
    "    rouge = Rouge()\n",
    "    return rouge.get_scores(model_out, reference, avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:03<00:00,  4.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model_out = []\n",
    "reference = []\n",
    "\n",
    "for pair in tqdm(hospitalityDataset, leave=True):\n",
    "    ans = qa({\n",
    "        'question': pair['question'],\n",
    "        'context': pair['context']\n",
    "    })\n",
    "    # append the prediction and reference to the respective lists\n",
    "    model_out.append(ans['answer'])\n",
    "    reference.append(pair['answer'])\n",
    "    ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deepset/bert-large-uncased-whole-word-masking-squad2 average exact match score 0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "em_score=compute_exact_match(model_out,reference)\n",
    "\n",
    "print(f\"Model {modelname} average exact match score {em_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deepset/bert-large-uncased-whole-word-masking-squad2 average F1-Score score 0.7055555514570986\n"
     ]
    }
   ],
   "source": [
    "rouge_score=compute_rouge_evaulation(model_out,reference)\n",
    "f1_score=rouge_score['rouge-l']['f']\n",
    "print(f\"Model {modelname} average F1-Score score {f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9aa8e61194e92d99b0a274c045d166271695586faf4a7ca05413957a358b22b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ptorchenv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
