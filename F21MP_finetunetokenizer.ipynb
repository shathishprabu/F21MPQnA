{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_squad(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    # initialize lists for contexts, questions, and answers\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    # iterate through all data in squad data\n",
    "    for group in squad_dict[\"data\"]:\n",
    "        for passage in group[\"paragraphs\"]:\n",
    "            context = passage[\"context\"]\n",
    "            for qa in passage[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "                if \"plausible_answers\" in qa.keys():\n",
    "                    access = \"plausible_answers\"\n",
    "                else:\n",
    "                    access = \"answers\"\n",
    "                for answer in qa[\"answers\"]:\n",
    "                    # append data to lists\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    # return formatted data lists\n",
    "    return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_contexts, train_questions, train_answers) = \\\n",
    "    read_squad('data/f21mp_test.json')\n",
    "    \n",
    "(val_contexts, val_questions, val_answers) = \\\n",
    "    read_squad('data/f21mp_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(train_contexts):\n",
    "    sample = sample.replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    if len(text_data) == 10_000:\n",
    "        # once we git the 10K mark, save to file\n",
    "        with open(f'data/tokenizer/hosp_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "# after saving in 10K chunks, we will have ~2082 leftover samples, we save those now too\n",
    "with open(f'data/tokenizer/hosp_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\tokenizer\\\\hosp_0.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "paths = [str(x) for x in Path('data/tokenizer/').glob('**/*.txt')]\n",
    "\n",
    "paths[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=8192, min_frequency=2,\n",
    "                show_progress=True,\n",
    "                special_tokens=[\n",
    "                                \"<s>\",\n",
    "                                \"<pad>\",\n",
    "                                \"</s>\",\n",
    "                                \"<unk>\",\n",
    "                                \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=579, model=ByteLevelBPE, add_prefix_space=False, lowercase=True, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_model(tokenizer_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Robertahospitality\\\\vocab.json', 'Robertahospitality\\\\merges.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#os.mkdir('Robertahospitality')\n",
    "\n",
    "tokenizer.save_model('Robertahospitality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizer using vocab.json and mrege.txt files\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.abspath(os.path.join('Robertahospitality','vocab.json')),\n",
    "    os.path.abspath(os.path.join('Robertahospitality','merges.txt'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the tokenizer\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'm', 'undo', '</s>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the tokens created\n",
    "tokenizer.encode(\"mundo\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 16    # input batch size for training (default: 64)\n",
    "VALID_BATCH_SIZE = 8    # input batch size for testing (default: 1000)\n",
    "TRAIN_EPOCHS = 15        # number of epochs to train (default: 10)\n",
    "LEARNING_RATE = 1e-4    # learning rate (default: 0.001)\n",
    "WEIGHT_DECAY = 0.01\n",
    "SEED = 42               # random seed (default: 42)\n",
    "MAX_LEN = 128\n",
    "SUMMARY_LEN = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 25 09:51:39 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 462.31       Driver Version: 462.31       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla M60          WDDM  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   39C    P8    17W / 150W |   7437MiB /  8192MiB |     13%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1084    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      1092    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      1432    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      1624    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A      3344    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      3876      C   ...envs\\ptorchenv\\python.exe    N/A      |\n",
      "|    0   N/A  N/A      5460    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      6236    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      6392    C+G   ...w5n1h2txyewy\\SearchUI.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check that we have a GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that PyTorch sees it\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=8192,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters:  49217282\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForQuestionAnswering\n",
    "\n",
    "model = RobertaForQuestionAnswering(config=config)\n",
    "print('Num parameters: ',model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file Robertahospitality\\config.json not found\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n",
      "file Robertahospitality\\config.json not found\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('Robertahospitality', max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test our tokenizer on a simple sentence\n",
    "tokens = tokenizer('Mundo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 408, 83, 2], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'M', 'undo', '</s>']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 16    # input batch size for training (default: 64)\n",
    "VALID_BATCH_SIZE = 8    # input batch size for testing (default: 1000)\n",
    "TRAIN_EPOCHS = 15        # number of epochs to train (default: 10)\n",
    "LEARNING_RATE = 1e-4    # learning rate (default: 0.001)\n",
    "WEIGHT_DECAY = 0.01\n",
    "SEED = 42               # random seed (default: 42)\n",
    "MAX_LEN = 128\n",
    "SUMMARY_LEN = 7"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9aa8e61194e92d99b0a274c045d166271695586faf4a7ca05413957a358b22b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ptorchenv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
